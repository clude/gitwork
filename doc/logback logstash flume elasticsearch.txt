logback-test.xml 


log collect
http://jasonwilder.com/blog/2012/01/03/centralized-logging/
http://jasonwilder.com/blog/2013/07/16/centralized-logging-architecture/
Log4j本来一统江湖好好的，后来被人说方法上太多同步修饰符，在高并发下性能太烂


地理空间距离
http://tech.meituan.com/lucene-distance.html
http://tech.meituan.com/solr-spatial-search.html


elasticsearch
http://www.cnblogs.com/huangfox/p/3543134.html
http://riching.iteye.com/blog/1921625
http://www.cnblogs.com/huangfox/p/3542858.html
http://www.searchly.com/documentation/developer-api-guide/java-jest/
http://www.656463.com/article/vU7jYr.htm

./plugin -install mobz/elasticsearch-head
http://xxx:9200/_plugin/head/
./plugin -install lukas-vlcek/bigdesk
http://xxx:9200/_plugin/bigdesk/#cluster

cluster.name：集群名称，es可以自我发现，拥有相同集群名字的es会构成集群。
node.name：节点名称，当前节点的名字。唯一。
node.master：是否允许当前节点成为master。
node.data：是否允许当前节点存储数据。
index.number_of_shards：一个索引默认的shard数量。
index.number_of_replicas：一个索引默认的副本数量。
path.data：数据存储.
path.log：日志存储。
bootstrap.mlockall：是否只使用内存（不使用swap）。
network.bind_host：设置绑定的ip地址，用于访问es。
network.publish_host：与其他node通信的地址，用于cluster间数据传输。

安装中文分词
index.analysis.analyzer.ik.type : "ik"
将ik config的ik目录copy到es config
copy ik.jar  httpclient common 若干jar到lib

http://www.jianshu.com/p/05cff717563c   Elasticsearch学习笔记
https://github.com/siddontang/elasticsearch-note
http://www.jianshu.com/p/cc6746ac4fc2   url清单
http://segmentfault.com/a/1190000002803609 安全
http://segmentfault.com/a/1190000000369962  js api
http://www.ttlsa.com/nginx/nginx-elasticsearch/   nginx
https://github.com/DmitryKey/luke    gui
http://dockone.io/article/505   我们为Elasticsearch安装了Hdfs Snapshot插件，可以定期将index 备份到Hdfs
https://github.com/garyelephant/blog/blob/master/elasticsearch_optimization_checklist.md   优化
https://www.elastic.co/blog/playing-http-tricks-nginx


http://www.infoq.com/cn/presentations/real-time-analysis-of-massive-operation-data  海量运维数据实时分析  elk


wget https://download.elasticsearch.org/logstash/logstash/logstash-1.3.3-flatjar.jar
wget 'http://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.4.3.tar.gz'
curl -X GET http://localhost:9200/  测试elasticsearch
直接到 elasticsearch data文件夹里删掉就行
 curl -X DELETE 'http://172.16.1.16:9200/logstash-2013.03.*' 
http://www.elasticsearch.org/guide/reference/api/delete/

然后运行 java -jar logstash-1.3.3-flatjar.jar agent -f logstash.conf 即可完成收集入库！ 再运行 java -jar logstash-1.3.3-flatjar.jar web 即可在9292端口访问到 Kibana 界面。
http://blog.csdn.net/cnbird2008/article/details/38762795
Open config/kibana.yml in an editor  Set the elasticsearch_url to point at your Elasticsearch instance
http://logstash.net/docs/1.4.2/tutorials/getting-started-with-logstash
http://logstash.net/docs/1.4.2/filters/grok
https://raw.githubusercontent.com/jordansissel/ruby-grok-ui/master/patterns/grok-patterns

http://my.oschina.net/guol/blog/179848
input { redis { host => "114.215.159.195"  data_type =>"list"  port => "6379"    key => "logstash"   type => "redis-input" }}
output { redis { host => "114.215.159.195"   port => "6379" data_type =>"list"  key => "logstash" } }
java -jar logstash-1.2.2-flatjar.jar agent -f logstash.agent.conf
./logstash -e 'input { file { path => "/data/www/webapp.atsmart.io/app/m5/logs/atsmart.log"  codec => multiline {pattern => "^%{TIMESTAMP_ISO8601} " negate => true   what => previous} } } filter{grok{ match => ["message" , "%{TIMESTAMP_ISO8601:datetime} %{INT:timestamp} %{NOTSPACE:thread} %{LOGLEVEL:level} %{JAVACLASS:class} %{ANY:content}" ] } } output { stdout { codec => "rubydebug"}   }'
./logstash -e 'input { file { path => "/data/www/webapp.atsmart.io/app/m5/logs/atsmart.log"  codec => multiline {pattern => "^%{TIMESTAMP_ISO8601} " negate => true   what => previous} } } filter{grok{ match => ["message" , "%{TIMESTAMP_ISO8601:datetime} %{INT:timestamp} %{NOTSPACE:thread} %{LOGLEVEL:level} %{JAVACLASS:class} %{ANY:content}" ] } } output {    redis { host => "114.215.159.195"   data_type =>"list"  key => "logstash" }  }'

([\s\S]*)同时，也可以用 “([\d\D]*)”、“([\w\W]*)” 来表示任意字符
ANY ([\s\S]*)

./logstash -e 'input { redis { host => "114.215.159.195"  data_type =>"list"  port => "6379"    key => "logstash"   type => "redis-input" }} output {   elasticsearch {   host => "127.0.0.1"   port => "9300"    }}'

Fluentd
http://www.oschina.net/question/12_33599
http://ju.outofmemory.cn/entry/102023
http://ju.outofmemory.cn/entry/95480
td-agent是Fluentd的稳定安装包  curl -L http://toolbelt.treasure-data.com/sh/install-redhat.sh | sh
type webhdfs   Fluentd新版本中添加了对HDFS的支持，out_webhdfs插件
http://m.oschina.net/blog/364018
目前最火的三类Agent：logslash，fluentd，flume；分别用jruby，ruby，java实现
http://eventlog.me/fluentd%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%B7%A5%E5%85%B7%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8/


flume
http://shiyanjun.cn/archives/915.html  flume
wget http://mirrors.hust.edu.cn/apache/flume/1.5.0/apache-flume-1.5.0-bin.tar.gz
/home/flume/bin/flume-ng agent --conf /home/flume/conf --conf-file /home/flume/conf/netcat.conf --name agent2 -Dflume.monitoring.type=http -Dflume.monitoring.port=34545   -Dflume.root.logger=DEBUG,console
http://shiyanjun.cn/archives/915.html 
https://cwiki.apache.org/confluence/display/FLUME/Getting+Started
http://abloz.com/2013/02/26/flume-channel-source-sink-summary.html
http://blog.csdn.net/yaoyasong/article/details/39400829
http://my.oschina.net/leejun2005/blog/288136
http://my.oschina.net/88sys/blog/71529
http://zzq635.blog.163.com/blog/static/19526448620139232113947/
https://blogs.apache.org/flume/entry/streaming_data_into_apache_hbase
http://blog.csdn.net/morning_pig/article/details/8534079
org.apache.flume.clients.log4jappender.Log4jAppender
host1.sinks.sink1.type = org.apache.flume.sink.hbase.AsyncHBaseSink
host1.sinks.sink1.type = org.apache.flume.sink.hbase.HBaseSink
org.apache.flume.sink.elasticsearch.ElasticSearchSink
source_agent.sources.apache_server.type = exec
source_agent.sources.apache_server.command = tail -F /opt/muse_tomcat/logs/localhost_access_log..txt
agent1.sources.source1.command = tail -n +0 -F /opt/tomcat/logs/catalina.out
To setup a multi-tier flow, you need to have an avro/thrift sink of first hop pointing to avro/thrift source of the next hop.  In order to flow the data across multiple agents or hops, the sink of the previous agent and source of the current hop need to be avro type with the sink pointing to the hostname (or IP address) and port of the source.